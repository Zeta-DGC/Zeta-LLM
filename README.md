
<img width="1920" height="1080" alt="Zeta Header Rounded" src="https://github.com/user-attachments/assets/9327c0d9-d30e-432c-a951-73a780f68fae" />

# Zeta LLM
| [Zeta Official X Account](https://x.com/Zeta_LLM/) |

Zeta is divided into three repositories.

- Anyone can create LLMs using the Zeta-Tool.
- Zeta-Dataset is the publication location of datasets for the Zeta-Tool.
- Azuki-Format is the format of datasets for the Zeta-Tool.

## Official Models [[Release Tab]](https://github.com/DiamondGotCat/Zeta/releases)

### [Zeta 4.5](https://github.com/DiamondGotCat/Zeta/releases/tag/zeta-4.5) (2025-06_01)
**(Latest)** | 464M Params | MIT License | Official GGUF Available (Quantized by mradermacher) |

### [Zeta 4](https://github.com/DiamondGotCat/Zeta/releases/tag/zeta-4) (2025-05_03)
| 464M Params | MIT License | Official GGUF Available (Quantized by mradermacher) |

### [Zeta 3](https://github.com/DiamondGotCat/Zeta/releases/tag/zeta-3) (2025-05_02)
| 464M Params | MIT License | Official GGUF Available (Quantized by mradermacher) |

### [Zeta 2](https://github.com/DiamondGotCat/Zeta/releases/tag/zeta-2) (2025-05_01)
**(Stable)** | 464M Params | MIT License | Official GGUF Available (Quantized by mradermacher) |

### [Zeta 1](https://github.com/DiamondGotCat/Zeta/releases/tag/zeta-1) (2025-04_01)
| 405M Params | MIT License | Official GGUF Available (Quantized by mradermacher) |

## Contributors
- [Sabale302](https://github.com/Sabale302) in Zeta-Tool: Thanks for sharing your ideas!
- [mradermacher](https://huggingface.co/mradermacher) in Zeta: Thanks for converting it to GGUF!

## Repos
- [Zeta-Tool](https://github.com/Zeta-LLM/Zeta-Tool): Create Your Own LLM using NLoRAT
- [Zeta-Dataset](https://github.com/Zeta-LLM/Zeta-Dataset/releases): Datasets for Zeta-Tool
- [Azuki-Format(AzukiF)](https://github.com/DiamondGotCat/Azuki-Format): Dataset Format for Zeta-Tool

(NOTE: Hinode-AI, and Azuki-AI is Marged to Zeta Project)

## What is NLoRAT?

**NLoRAT (Non-LoRA Tuning)** is a simple method of fine-tuning an existing LLM *without* using LoRA or adapters.  
You just train the model as-is, using your own dataset (e.g. in Azuki Format).  
No special tools or architecture changes are required.

If you can run the original LLM, you can also run the LLM with NLoRAT.
